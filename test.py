import requests
from bs4 import BeautifulSoup
import json
import os
from urllib.parse import urljoin
from datetime import datetime
import time

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª
CONFIG = {
    'SEEN_ADS_FILE': 'seen_ads.json',
    'OUTPUT_FILE': 'new_ads.txt',
    'SEARCH_URLS': [
        'https://divar.ir/s/tehran/transport-delivery-jobs?q=%D8%A7%D8%B3%D9%86%D9%BE%20%D8%A8%D8%A7%DA%A9%D8%B3',
        'https://divar.ir/s/tehran/transport-delivery-jobs?q=%D9%BE%DB%8C%DA%A9%20%D9%85%D9%88%D8%AA%D9%88%D8%B1%DB%8C%20%D9%85%DB%8C%D8%A7%D8%B1%D9%87',
        'https://divar.ir/s/tehran/transport-delivery-jobs?q=%D9%BE%DB%8C%DA%A9%20%D9%85%D9%88%D8%AA%D9%88%D8%B1%DB%8C%20%D8%A7%D8%B3%D9%86%D9%BE'
    ],
    'HEADERS': {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    },
    'REQUEST_DELAY': 3,
    'MAX_ADS': 5
}

def load_seen_ads():
    """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù„ÛŒØ³Øª Ø¢Ú¯Ù‡ÛŒâ€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡"""
    try:
        if os.path.exists(CONFIG['SEEN_ADS_FILE']):
            with open(CONFIG['SEEN_ADS_FILE'], 'r') as f:
                return json.load(f)
        return []
    except Exception as e:
        print(f'âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù„ÛŒØ³Øª Ø¢Ú¯Ù‡ÛŒâ€ŒÙ‡Ø§: {e}')
        return []

def save_seen_ads(seen_ads):
    """Ø°Ø®ÛŒØ±Ù‡ Ù„ÛŒØ³Øª Ø¢Ú¯Ù‡ÛŒâ€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡"""
    try:
        with open(CONFIG['SEEN_ADS_FILE'], 'w') as f:
            json.dump(seen_ads, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f'âŒ Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡ Ù„ÛŒØ³Øª Ø¢Ú¯Ù‡ÛŒâ€ŒÙ‡Ø§: {e}')

def get_ad_links(url):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø¢Ú¯Ù‡ÛŒ Ø§Ø² ØµÙØ­Ù‡ Ø¬Ø³ØªØ¬Ùˆ"""
    try:
        response = requests.get(url, headers=CONFIG['HEADERS'])
        soup = BeautifulSoup(response.text, 'html.parser')
        links = []
        
        for card in soup.select('a.kt-post-card__action'):
            relative_link = card.get('href')
            if relative_link:
                absolute_link = urljoin('https://divar.ir', relative_link)
                links.append(absolute_link)
        
        return links[:CONFIG['MAX_ADS']]
    
    except Exception as e:
        print(f'âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ Ø§Ø² {url}: {e}')
        return []

def scrape_ad_page(ad_url):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ø²Ø¦ÛŒØ§Øª Ø§Ø² ØµÙØ­Ù‡ Ø¢Ú¯Ù‡ÛŒ"""
    try:
        time.sleep(CONFIG['REQUEST_DELAY'])
        response = requests.get(ad_url, headers=CONFIG['HEADERS'])
        soup = BeautifulSoup(response.text, 'html.parser')

        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¹Ù†ÙˆØ§Ù† Ø¨Ø§ Ù‡Ù†Ø¯Ù„ÛŒÙ†Ú¯ Ø®Ø·Ø§
        title = "Ø¹Ù†ÙˆØ§Ù† ÛŒØ§ÙØª Ù†Ø´Ø¯"
        try:
            title_element = soup.find('h1', class_='kt-page-title__title')
            if title_element:
                title = title_element.text.strip()
        except AttributeError:
            pass

        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙˆØ¶ÛŒØ­Ø§Øª Ø¨Ø§ Ù‡Ù†Ø¯Ù„ÛŒÙ†Ú¯ Ø®Ø·Ø§
        description = "ØªÙˆØ¶ÛŒØ­Ø§Øª ÛŒØ§ÙØª Ù†Ø´Ø¯"
        try:
            description_div = soup.find('div', class_='kt-description-row')
            if description_div:
                description_p = description_div.find('p', class_='kt-description-row__text--primary')
                if description_p:
                    description = '\n'.join(
                        [line.strip() 
                         for line in description_p.text.split('\n') 
                         if line.strip()]
                    )
        except AttributeError:
            pass

        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ID
        ad_id = ad_url.split('/')[-1]

        return {
            'id': ad_id,
            'title': title,
            'description': description,
            'link': ad_url
        }

    except Exception as e:
        print(f'âŒ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¢Ú¯Ù‡ÛŒ {ad_url}: {e}')
        return None

def save_results(ads):
    """Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬ Ø¯Ø± ÙØ§ÛŒÙ„ Ù…ØªÙ†ÛŒ"""
    try:
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        with open(CONFIG['OUTPUT_FILE'], 'w', encoding='utf-8') as f:
            f.write(f"=== Ø¢Ú¯Ù‡ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ ({timestamp}) ===\n\n")
            for idx, ad in enumerate(ads, 1):
                f.write(f"Ø¢Ú¯Ù‡ÛŒ #{idx}\n")
                f.write(f"Ø¹Ù†ÙˆØ§Ù†: {ad['title']}\n")
                f.write(f"ØªÙˆØ¶ÛŒØ­Ø§Øª:\n{ad['description']}\n")
                f.write(f"Ù„ÛŒÙ†Ú©: {ad['link']}\n")
                f.write("-"*50 + "\n\n")
        print(f'âœ… Ù†ØªØ§ÛŒØ¬ Ø¯Ø± {CONFIG["OUTPUT_FILE"]} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯')
    except Exception as e:
        print(f'âŒ Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬: {e}')

def main():
    seen_ads = load_seen_ads()
    new_ads = []

    # Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§
    all_links = []
    for url in CONFIG['SEARCH_URLS']:
        print(f'ğŸ” Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø±Ø±Ø³ÛŒ Ø¢Ø¯Ø±Ø³: {url}')
        links = get_ad_links(url)
        all_links.extend(links)
        print(f'ğŸ“ ØªØ¹Ø¯Ø§Ø¯ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ ÛŒØ§ÙØª Ø´Ø¯Ù‡: {len(links)}')

    # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¢Ú¯Ù‡ÛŒâ€ŒÙ‡Ø§
    for link in all_links:
        ad_id = link.split('/')[-1]
        
        if ad_id in seen_ads:
            print(f'â© Ø¢Ú¯Ù‡ÛŒ {ad_id} Ù‚Ø¨Ù„Ø§Ù‹ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡')
            continue
            
        print(f'ğŸ”„ Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¢Ú¯Ù‡ÛŒ: {link}')
        ad_data = scrape_ad_page(link)
        
        if ad_data:
            new_ads.append(ad_data)
            seen_ads.append(ad_id)
            print(f'âœ”ï¸ Ø¢Ú¯Ù‡ÛŒ {ad_id} Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯')

    # Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬
    if new_ads:
        save_results(new_ads)
        save_seen_ads(seen_ads)
        print(f'\nğŸ‰ ØªØ¹Ø¯Ø§Ø¯ Ø¢Ú¯Ù‡ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯: {len(new_ads)}')
    else:
        print('\nâš ï¸ Ù‡ÛŒÚ† Ø¢Ú¯Ù‡ÛŒ Ø¬Ø¯ÛŒØ¯ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯')

if __name__ == '__main__':
    main()